import os
import time
import re
import csv
import random
import datetime
from io import BytesIO
from difflib import SequenceMatcher

from dotenv import load_dotenv
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import NoSuchElementException
from bs4 import BeautifulSoup
from PIL import ImageGrab
import pyautogui
import pyperclip
import keyboard
import requests
import pandas as pd
import FinanceDataReader as fdr
from tqdm import tqdm
import google.generativeai as genai

# â¬‡ï¸ í™˜ê²½ë³€ìˆ˜ ë¡œë”©
load_dotenv()
NAVER_ID = os.getenv("NAVER_ID")
NAVER_PW = os.getenv("NAVER_PW")
UNSPLASH_ACCESS_KEY = os.getenv("UNSPLASH_ACCESS_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel("gemini-pro")

# âœ… ì£¼ì œ ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°
def image_url(query):
    url = f"https://api.unsplash.com/search/photos?page={random.randint(1, 3)}&query={query}"
    headers = {'Authorization': f'Client-ID {UNSPLASH_ACCESS_KEY}'}
    response = requests.get(url, headers=headers)
    try:
        return response.json()['results'][0]['urls']['regular']
    except:
        return None

# âœ… ê¸°ì—…ëª… ìœ ì‚¬ë„ ë¹„êµ
def extract_company_name(title, company_list):
    max_similarity = 0.15
    most_similar = None
    for company in company_list:
        similarity = SequenceMatcher(None, title, company).ratio()
        if similarity > max_similarity:
            max_similarity = similarity
            most_similar = company
    return most_similar

# âœ… ê¸°ì‚¬ ì¤‘ë³µ ì œê±° ë° ì°¨íŠ¸ ìº¡ì²˜
def remove_duplicate_articles(news_data, company_list, df_krx):
    seen = set()
    unique_articles = []
    numbering = 0
    codes = []

    for data in news_data:
        title = data[0]
        company = extract_company_name(title, company_list)
        try:
            code = df_krx[df_krx['Name'] == company]['Code'].values[0]
        except:
            continue

        if company and company not in seen:
            seen.add(company)
            unique_articles.append(data)
            codes.append(code)

            url = f"https://finance.naver.com/item/fchart.naver?code={code}"
            driver = webdriver.Chrome()
            driver.maximize_window()
            driver.get(url)
            time.sleep(2)

            pyautogui.moveTo(1913, 1020, duration=1)
            pyautogui.click(clicks=5)
            chart_element = driver.find_element(By.XPATH, '//*[@id="content"]/div[2]/cq-context/div[2]')
            chart_element.screenshot(f"dailychart_{numbering}_final.png")
            driver.quit()

            numbering += 1

    return unique_articles

# âœ… Geminië¥¼ ì´ìš©í•œ ë‰´ìŠ¤ ìš”ì•½
def summarize_articles_with_gemini(articles):
    summaries = []
    for article in tqdm(articles):
        prompt = f"""
ë‹¤ìŒì€ ì£¼ì‹ ë‰´ìŠ¤ ê¸°ì‚¬ì…ë‹ˆë‹¤. ì¡´ëŒ“ë§ë¡œ ê¸¸ê²Œ í’€ì´í•´ ì£¼ì„¸ìš”:
"{article[2]}"
"""
        response = model.generate_content(prompt)
        summaries.append(response.text.strip())
    return summaries

# âœ… ë‰´ìŠ¤ ìˆ˜ì§‘
def get_news_data():
    url = "https://search.naver.com/search.naver?where=news&query=%ED%8A%B9%EC%A7%95%EC%A3%BC&pd=1&nso=so:dd,p:1w"
    page = requests.get(url)
    soup = BeautifulSoup(page.content, 'html.parser')
    news_data = []

    for item in soup.find_all('li', class_='bx'):
        title_tag = item.find('a', class_='news_tit')
        if not title_tag: continue
        title = title_tag.get('title')
        href = title_tag.get('href')

        summary_tag = item.find('div', class_='dsc_wrap')
        summary = summary_tag.get_text(strip=True) if summary_tag else ''

        time_tag = item.find('span', class_='info')
        if time_tag and '1ì¼ ì „' not in time_tag.get_text():
            news_data.append((title, href, summary))
    return news_data

# âœ… ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìë™ ë¡œê·¸ì¸ ë° í¬ìŠ¤íŒ…
def post_to_naver_blog(articles, summaries):
    site_url = "https://nid.naver.com/nidlogin.login"
    driver = webdriver.Chrome()
    driver.maximize_window()
    driver.get(site_url)
    time.sleep(5)

    pyperclip.copy(NAVER_ID)
    driver.find_element(By.NAME, 'id').send_keys(Keys.CONTROL, 'v')
    pyperclip.copy(NAVER_PW)
    driver.find_element(By.NAME, 'pw').send_keys(Keys.CONTROL, 'v')
    driver.find_element(By.ID, "log.login").click()
    time.sleep(5)

    # ë¸”ë¡œê·¸ ì ‘ê·¼
    driver.get("https://blog.naver.com")
    time.sleep(5)
    driver.find_element(By.CLASS_NAME, 'btn_check').click()
    time.sleep(2)
    driver.find_element(By.XPATH, '//*[@id="root"]/header/div[2]/button/i').click()
    time.sleep(2)
    driver.find_elements(By.CLASS_NAME, 'icon___T7KP')[4].click()
    time.sleep(5)

    # ê¸€ ì œëª© ì‘ì„±
    now = datetime.datetime.now()
    today = now.strftime("%m/%d")
    title = f"[{today}] êµ­ë‚´ í…Œë§ˆì£¼ & íŠ¹ì§•ì£¼"
    driver.find_element(By.XPATH, '//*[@id="se_component_wrapper"]/div[1]/div/div[3]/div/div/div/div/textarea').send_keys(title)

    # ì²« ë¬¸ë‹¨ + ì´ë¯¸ì§€
    pyautogui.moveTo(27, 205); pyautogui.click()
    time.sleep(1)
    pyautogui.moveTo(74, 205); pyautogui.click()
    time.sleep(1)
    pyautogui.moveTo(85, 777); pyautogui.click()
    pyautogui.moveTo(259, 286); pyautogui.doubleClick()
    pyautogui.moveTo(293, 972); pyautogui.click()
    keyboard.write("unsplash_images_theme.jpg")
    pyautogui.press("enter")
    time.sleep(3)

    # ë³¸ë¬¸ ì…ë ¥
    for i, (article, summary) in enumerate(zip(articles, summaries), 1):
        comment_field = driver.find_element(By.XPATH, f'//*[@id="se_component_wrapper"]/div[{i*3}]/div/div/div/div[2]/div/div/div/div')
        comment_field.send_keys(f"{i}. {article[0]}\n{article[1]}")
        time.sleep(1)

        # ì´ë¯¸ì§€ ì²¨ë¶€
        pyautogui.moveTo(27, 150); pyautogui.click()
        pyautogui.moveTo(74, 150); pyautogui.click()
        pyautogui.moveTo(85, 777); pyautogui.click()
        pyautogui.moveTo(259, 286); pyautogui.doubleClick()
        pyautogui.moveTo(293, 972); pyautogui.click()
        image_name = f"dailychart_{i-1}_final.png"
        keyboard.write(image_name)
        pyautogui.press("enter")
        time.sleep(2)

        # ìš”ì•½ ì¶”ê°€
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        pyperclip.copy(summary)
        pyautogui.hotkey("ctrl", "v")
        pyautogui.hotkey("enter")

    driver.quit()

# âœ… ë©”ì¸ ì‹¤í–‰ ë¡œì§
if __name__ == "__main__":
    print("ğŸ“Œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    news_data = get_news_data()

    print("ğŸ“Œ ìƒì¥ ê¸°ì—… ëª©ë¡ ë¡œë”©...")
    df_krx = fdr.StockListing("KRX")
    company_list = df_krx["Name"].tolist()

    print("ğŸ“Œ ì¤‘ë³µ ê¸°ì‚¬ í•„í„°ë§ ë° ì°¨íŠ¸ ìº¡ì²˜ ì¤‘...")
    unique_articles = remove_duplicate_articles(news_data, company_list, df_krx)

    print("ğŸ“Œ Geminië¡œ ìš”ì•½ ì¤‘...")
    summaries = summarize_articles_with_gemini(unique_articles)

    print("ğŸ“Œ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìë™ í¬ìŠ¤íŒ… ì¤‘...")
    post_to_naver_blog(unique_articles, summaries)
